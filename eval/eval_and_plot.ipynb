{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T08:17:34.348761900Z",
     "start_time": "2024-02-02T08:17:34.345148100Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "#os.environ[\"MODIN_ENGINE\"] = \"ray\"  # Modin will use Ray\n",
    "#import modin.pandas as pd\n",
    "#import ray\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#ray.init(ignore_reinit_error=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T08:17:34.521475900Z",
     "start_time": "2024-02-02T08:17:34.347741300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "%matplotlib inline\n",
    "sns.set(font_scale=1.25)\n",
    "# plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "matplotlib.rcParams.update({'figure.autolayout': True})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T08:17:35.198364400Z",
     "start_time": "2024-02-02T08:17:34.525527900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "raptor_dist = [0, 10241, 491582, 712794, 831695, 831695, 831695, 831695, 831695, 831695, 948446, 1032189, 1032189,\n",
    "               1032189, 1032189, 1032189, 1032189, 1032189, 1032189, 1032189, 1032189, 1032189, 1032189, 1032189,\n",
    "               1032189, 1032189, 1032189, 1032189, 1032189, 1032189, 1032189, 1032189, 1032189, 1032189, 1032189,\n",
    "               1032189, 1032189, 1032189, 1032189, 1032189, 1048576]\n",
    "\n",
    "\n",
    "def load_json(filename):\n",
    "    with open(filename) as f:\n",
    "        json_diff = json.load(f)\n",
    "    return json_diff\n",
    "\n",
    "\n",
    "from Helper import encode, to_dist_list, norm_list\n",
    "import Distribution\n",
    "\n",
    "\n",
    "def recalculate_for_dist(df, seed_spacing=0, use_payload_xor=False, use_raptor_dist=False):\n",
    "    files = df[\"file_lst\"]\n",
    "    chunksize = df[\"chunksize\"]\n",
    "    overhead_fac = df[\"overhead_fac\"]\n",
    "    avg_error_fac = df[\"avg_error_fac\"]\n",
    "    clean_deg_len_fac = df[\"clean_deg_len_fac\"]\n",
    "    clean_avg_error_fac = df[\"clean_avg_error_fac\"]\n",
    "    non_unique_packets_fac = df.get(\"non_unique_packets_fac\")\n",
    "    # df[\"unique_packets_fac\"]\n",
    "    seed_spacing = df.get(\"seed_space\", seed_spacing)\n",
    "    use_payload_xor = df.get(\"use_payload_xor\", use_payload_xor)\n",
    "    unrecovered_packets_fac = df[\"unrecovered_packets_fac\"]\n",
    "    repeats = df[\"repeats\"]\n",
    "    if \"finished_prev_best\" in df:\n",
    "        dist_func = df[\"finished_prev_best\"][\"dist_lst\"]\n",
    "    else:\n",
    "        dist_func = df[\"finished_gen\"][-1][\"dist_lst\"]\n",
    "    if use_raptor_dist:\n",
    "        dist_func = norm_list(to_dist_list(raptor_dist))\n",
    "\n",
    "    # create all packets for the dist:\n",
    "    dist = Distribution.Distribution(dist_func, overhead_fac=overhead_fac,\n",
    "                                     avg_error_fac=avg_error_fac, clean_deg_len_fac=clean_deg_len_fac,\n",
    "                                     clean_avg_error_fac=clean_avg_error_fac,\n",
    "                                     non_unique_packets_fac=non_unique_packets_fac,\n",
    "                                     unrecovered_packets_fac=unrecovered_packets_fac, seed_spacing=seed_spacing,\n",
    "                                     use_payload_xor=use_payload_xor)\n",
    "\n",
    "    dist.compute_fitness(files, repeats, chunksize)\n",
    "    dist.calculate_error_value()\n",
    "    return dist"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Set correct parameter below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# ! IMPORTANT: the \"results\" folder is NOT part of the repository, as it contains the results of the experiments!\n",
    "## It can be downloaded from this repository: https://github.com/umr-ds/OFC4DNA_results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T08:17:40.991799300Z",
     "start_time": "2024-02-02T08:17:35.199939Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "base_dir = \"./results\"\n",
    "#working_dir = \"results_DornRapunzel\"\n",
    "#working_dir = \"results_no_improv_new\"\n",
    "#working_dir = \"results_final_same_params\"\n",
    "working_dir = \"results_bmp_low_entropy\"\n",
    "#filename = \"results_evo/evo_opt_state.json\"\n",
    "filename = \"results_diff/diff_opt_state.json/diff_opt_state.json\"\n",
    "img_format = \"_10_5\" # only \"_10_5\" or \"\" supported by now...\n",
    "\n",
    "# possible combinations:\n",
    "# working_dir x filename x img_format (4 x 2 x 2 = 16 combinations currently used)\n",
    "\n",
    "if \"evo_opt\" in filename:\n",
    "    opt_type = \"evolutionary optimization\"\n",
    "elif \"GrdOpt\" in filename:\n",
    "    opt_type = \"gradient descent\"  # TODO: check!\n",
    "elif \"results_diff\" in filename:\n",
    "    opt_type = \"differential evolution\"\n",
    "else:\n",
    "    opt_type = \"unknown optimization type\"\n",
    "\n",
    "json_diff = load_json(f\"{base_dir}/{working_dir}/{filename}\")\n",
    "# we should use more samples for the default raptor dist to even out outliers for the overhead (however, changing the sample count to a value other that the value used during the experiment is not a good approach either!)\n",
    "\n",
    "# WARNING: ensure that ALL rules match the rules used during the optimization, otherwise the results will be wrong!\n",
    "#raptor_res = recalculate_for_dist(json_diff, use_raptor_dist=True).to_json()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T08:17:53.354795400Z",
     "start_time": "2024-02-02T08:17:40.999945600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\n",
    "# Overhead and Synthetic Error Value for Evolutionary Optimization\n",
    "if img_format != \"\":\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "else:\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_title(f\"Overhead and synthetic error value for {opt_type}\")\n",
    "ax1.set_xlabel(\"Generation\")\n",
    "ax1.set_ylabel(\"Average synthetic error\")\n",
    "df_diff = pd.DataFrame(json_diff[\"gen_calculated_error\"])\n",
    "df_diff = pd.concat([pd.DataFrame([raptor_res])[\"calculated_error_value\"], df_diff])\n",
    "df_diff = df_diff.reset_index(drop=True)\n",
    "ax1.plot(df_diff, color=\"tab:blue\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Average overhead\")\n",
    "df_diff = pd.DataFrame(json_diff[\"gen_avg_over\"])\n",
    "df_diff = pd.concat([pd.DataFrame([raptor_res])[\"overhead\"], df_diff])\n",
    "df_diff = df_diff.reset_index(drop=True)\n",
    "# df_diff = pd.DataFrame(json_diff[\"gen_avg_err\"])\n",
    "ax2.plot(df_diff, color=\"tab:red\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(f\"{working_dir}_{filename.split('/')[0]}_results_overhead{img_format}.svg\", format=\"svg\", dpi=1200)\n",
    "plt.savefig(f\"{working_dir}_{filename.split('/')[0]}_results_overhead{img_format}.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "# Non unique packets and synthetic error value for Evolutionary Optimization\n",
    "\n",
    "if img_format != \"\":\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "else:\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_title(f\"Non unique packets and synthetic error value for {opt_type}\")\n",
    "ax1.set_xlabel(\"Generation\")\n",
    "ax1.set_ylabel(\"Average synthetic error\")\n",
    "df_diff = pd.DataFrame(json_diff[\"gen_calculated_error\"])\n",
    "df_diff = pd.concat([pd.DataFrame([raptor_res])[\"calculated_error_value\"], df_diff])\n",
    "df_diff = df_diff.reset_index(drop=True)\n",
    "df_diff_calc_err = df_diff\n",
    "ax1.plot(df_diff, color=\"tab:blue\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Non unique packets\")\n",
    "df_a = pd.DataFrame(json_diff[\"gen_best_dist\"])\n",
    "df_a = pd.concat([pd.DataFrame([raptor_res]), df_a])\n",
    "df_a = df_a.reset_index(drop=True)\n",
    "df_diff = pd.DataFrame(df_a[\"non_unique_packets\"])\n",
    "\n",
    "ax2.plot(df_diff, color=\"tab:red\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(f\"{working_dir}_{filename.split('/')[0]}_results_non_unique{img_format}.svg\", format=\"svg\", dpi=1200)\n",
    "plt.savefig(f\"{working_dir}_{filename.split('/')[0]}_results_non_unique{img_format}.pdf\", bbox_inches=\"tight\")\n",
    "print(f\"Saved as {working_dir}_{filename.split('/')[0]}_results_non_unique{img_format}.svg/pdf\")\n",
    "\n",
    "# rule abiding packets and synthetic error value for Evolutionary Optimization\n",
    "if img_format != \"\":\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "else:\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_title(f\"Rule abiding packets and synthetic error value for {opt_type}\")\n",
    "ax1.set_xlabel(\"Generation\")\n",
    "ax1.set_ylabel(\"Average synthetic error\")\n",
    "df_diff = pd.DataFrame(json_diff[\"gen_calculated_error\"])\n",
    "df_diff = pd.concat([pd.DataFrame([raptor_res])[\"calculated_error_value\"], df_diff])\n",
    "df_diff_calc_err = df_diff\n",
    "ax1.plot(df_diff, color=\"tab:blue\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Rule abiding packets\")\n",
    "df_diff = pd.DataFrame(df_a[\"clean_deg_len\"])\n",
    "ax2.plot(df_diff, color=\"tab:red\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(f\"{working_dir}_{filename.split('/')[0]}_results_rule_abiding_packet{img_format}.svg\", format=\"svg\",\n",
    "            dpi=1200)\n",
    "plt.savefig(f\"{working_dir}_{filename.split('/')[0]}_results_rule_abiding_packet{img_format}.pdf\", bbox_inches=\"tight\")\n",
    "print(f\"Saved as {working_dir}_{filename.split('/')[0]}_results_rule_abiding_packet{img_format}.svg/pdf\")\n",
    "\n",
    "# avg_unrecovered packets and synthetic error value for Evolutionary Optimization\n",
    "if img_format != \"\":\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "else:\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_title(f\"Average unrecoverable chunks and synthetic error value for {opt_type}\")\n",
    "ax1.set_xlabel(\"Generation\")\n",
    "ax1.set_ylabel(\"Average synthetic error\")\n",
    "df_diff = pd.DataFrame(json_diff[\"gen_calculated_error\"])\n",
    "df_diff = pd.concat([pd.DataFrame([raptor_res])[\"calculated_error_value\"], df_diff])\n",
    "df_diff = df_diff.reset_index(drop=True)\n",
    "df_diff_calc_err = df_diff\n",
    "ax1.plot(df_diff, color=\"tab:blue\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Average unrecoverable chunks\")\n",
    "df_diff = pd.DataFrame(df_a[\"avg_unrecovered\"])\n",
    "ax2.plot(df_diff, color=\"tab:red\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(f\"{working_dir}_{filename.split('/')[0]}_results_avg_unrecovered{img_format}.svg\", format=\"svg\", dpi=1200)\n",
    "plt.savefig(f\"{working_dir}_{filename.split('/')[0]}_results_avg_unrecovered{img_format}.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "print(f\"Saved as {working_dir}_{filename.split('/')[0]}_results_avg_unrecovered{img_format}.svg/pdf\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## plot the optimized distribution function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T08:17:54.075163Z",
     "start_time": "2024-02-02T08:17:53.369883100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "if img_format != \"\":\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "else:\n",
    "    fig, ax1 = plt.subplots()\n",
    "ax1.set_title(\"Optimized distribution function\")\n",
    "ax1.set_xlabel(\"Degree\")\n",
    "ax1.set_ylabel(\"Probability\")\n",
    "# ax1.plot(df_diff, color=\"tab:blue\")\n",
    "ax1.plot(json_diff[\"gen_best_dist\"][-1][\"dist_lst\"])\n",
    "print(json_diff[\"gen_best_dist\"][-1][\"dist_lst\"])\n",
    "plt.savefig(f\"{working_dir}_{filename.split('/')[0]}_results_diff_opt_dist{img_format}.svg\", format=\"svg\", dpi=1200)\n",
    "plt.savefig(f\"{working_dir}_{filename.split('/')[0]}_results_diff_opt_dist{img_format}.pdf\", bbox_inches=\"tight\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T08:17:55.267122200Z",
     "start_time": "2024-02-02T08:17:54.077198500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "if img_format != \"\":\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "else:\n",
    "    fig, ax1 = plt.subplots()\n",
    "ax1.set_title(\"Average error probability for each degree\")\n",
    "ax1.set_xlabel(\"Degree\")\n",
    "ax1.set_ylabel(\"Distribution (probability)\")\n",
    "# df_diff = pd.DataFrame(json_diff[\"gen_best_dist\"][-1][\"dist_lst\"])\n",
    "ax1.plot(json_diff[\"gen_best_dist\"][-1][\"dist_lst\"], color=\"tab:blue\")\n",
    "\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Error probability\")\n",
    "ax2.plot(json_diff[\"gen_best_dist\"][-1][\"degree_errs\"], color=\"tab:red\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(f\"{working_dir}_{filename.split('/')[0]}_results_degree_vs_degree_error{img_format}.svg\", format=\"svg\", dpi=1200)\n",
    "plt.savefig(f\"{working_dir}_{filename.split('/')[0]}_results_degree_vs_degree_error{img_format}.pdf\", bbox_inches=\"tight\")\n",
    "print(f\"Saved as {working_dir}_{filename.split('/')[0]}_results_degree_vs_degree_error{img_format}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Comparision of the different optimization algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T08:17:55.374140300Z",
     "start_time": "2024-02-02T08:17:55.269198600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes, mark_inset\n",
    "\n",
    "base_dir = \"./results\"\n",
    "# working dir and prefix for algorithm comparison:\n",
    "working_dir = \"results_bmp_low_entropy\"\n",
    "working_dir = \"cmp_results\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T08:18:03.475370600Z",
     "start_time": "2024-02-02T08:17:55.373125Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# calculate raptor res for using the same optimization rules as in the json:\n",
    "raptor_res = None\n",
    "dir_prefix = \"evo_cmp_mut\"\n",
    "\n",
    "# create list with all folders starting with \"EvAlg\":\n",
    "folders = [f for f in os.listdir(f\"{base_dir}/{working_dir}\") if f.startswith(dir_prefix)]\n",
    "for include_raptor in [True, False]:\n",
    "    res = {}\n",
    "\n",
    "    if img_format != \"\":\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    else:\n",
    "        fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.set_title(\"Impact of the mutation rate on the evolutionary optimization\")\n",
    "    ax1.set_xlabel(\"Generation\")\n",
    "    ax1.set_ylabel(\"Synthetic error\")\n",
    "\n",
    "    for folder in sorted(folders):\n",
    "        print(folder)\n",
    "        # load csv \"_ev_optimization_log.csv\":\n",
    "        content = load_json(f\"{base_dir}/{working_dir}/{folder}/evo_opt_state.json\")\n",
    "        if raptor_res is None:\n",
    "            raptor_res = recalculate_for_dist(content, use_raptor_dist=True).to_json()\n",
    "        gen_calc_err = pd.DataFrame(content[\"gen_calculated_error\"])\n",
    "        if include_raptor:\n",
    "            #df_diff = pd.concat([pd.DataFrame([raptor_res])[\"calculated_error_value\"], df_diff])\n",
    "            gen_calc_err = pd.concat([pd.DataFrame([raptor_res])[\"calculated_error_value\"], gen_calc_err])\n",
    "        res[folder.replace(dir_prefix + \"_\", \"\")] = content\n",
    "        ax1.plot(gen_calc_err, label=folder.replace(dir_prefix + \"_\", \"\"))\n",
    "        # show labels in a legend:\n",
    "        #ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "    #extent = (0, 512, 0, 512)\n",
    "    if include_raptor:  # subregion of the original image\n",
    "        axins = zoomed_inset_axes(ax1, 4, loc=\"upper center\")\n",
    "        #ax1.inset_axes(\n",
    "        #[0, 100, 0, 50],\n",
    "        #xlim=(x1, x2), ylim=(y1, y2), xticklabels=[], yticklabels=[])\n",
    "\n",
    "        gen_calc_err_min = 8000\n",
    "        gen_calc_err_min_max = -8000\n",
    "        for name in sorted(res):\n",
    "            gen_calc_err = pd.DataFrame(res[name][\"gen_calculated_error\"])\n",
    "            gen_calc_err = pd.concat([pd.DataFrame([raptor_res])[\"calculated_error_value\"], gen_calc_err])\n",
    "            gen_calc_err = gen_calc_err.reset_index(drop=True)\n",
    "            curr_min = min(gen_calc_err.values)\n",
    "            gen_calc_err_min = curr_min if curr_min < gen_calc_err_min else gen_calc_err_min\n",
    "            gen_calc_err_min_max = curr_min if curr_min > gen_calc_err_min_max else gen_calc_err_min_max\n",
    "            axins.plot(gen_calc_err, label=name.replace(dir_prefix + \"_\", \"\"))\n",
    "\n",
    "        axins.set_xlim(0, 20)\n",
    "        lower_bound = (gen_calc_err_min // 100) * 100\n",
    "        upper_bound = (gen_calc_err_min_max // 100) * 100 + 900\n",
    "        axins.set_ylim(lower_bound, upper_bound)\n",
    "        mark_inset(ax1, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
    "        #ax1.indicate_inset_zoom(axins, edgecolor=\"black\")\n",
    "    #fig.tight_layout()\n",
    "    ax1.legend()\n",
    "    plt.draw()\n",
    "\n",
    "    plt.savefig(f\"{'including_raptor_' if include_raptor else ''}{working_dir}_{dir_prefix}{img_format}.svg\",\n",
    "                format=\"svg\", dpi=1200)\n",
    "    plt.savefig(f\"{'including_raptor_' if include_raptor else ''}{working_dir}_{dir_prefix}{img_format}.pdf\",\n",
    "                bbox_inches=\"tight\")\n",
    "    print(f\"Saved as {'including_raptor_' if include_raptor else ''}{working_dir}_{dir_prefix}{img_format}.svg/pdf\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T08:18:11.172782900Z",
     "start_time": "2024-02-02T08:18:03.463136800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# create list with all folders starting with \"EvAlg\":\n",
    "dir_prefix = \"evo_cmp_pop\"\n",
    "folders = [f for f in os.listdir(f\"{base_dir}/{working_dir}\") if f.startswith(dir_prefix)]\n",
    "folders = [int(folder.replace(dir_prefix + \"_\", \"\")) for folder in folders]\n",
    "\n",
    "# calculate raptor res for using the same optimization rules as in the json:\n",
    "raptor_res = None\n",
    "\n",
    "for include_raptor in [True, False]:\n",
    "    res = {}\n",
    "\n",
    "    if img_format != \"\":\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    else:\n",
    "        fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.set_title(\"Impact of the population size on the evolutionary optimization\")\n",
    "    ax1.set_xlabel(\"Generation\")\n",
    "    ax1.set_ylabel(\"Average synthetic error\")\n",
    "    for folder in sorted(folders):\n",
    "        print(folder)\n",
    "        # load csv \"_ev_optimization_log.csv\":\n",
    "        content = load_json(f\"{base_dir}/{working_dir}/{dir_prefix}_{folder}/evo_opt_state.json\")\n",
    "        if raptor_res is None:\n",
    "            raptor_res = recalculate_for_dist(content, use_raptor_dist=True).to_json()\n",
    "        gen_calc_err = pd.DataFrame(content[\"gen_calculated_error\"])\n",
    "        if include_raptor:\n",
    "            #df_diff = pd.concat([pd.DataFrame([raptor_res])[\"calculated_error_value\"], df_diff])\n",
    "            gen_calc_err = pd.concat([pd.DataFrame([raptor_res])[\"calculated_error_value\"], gen_calc_err])\n",
    "        res[folder] = content\n",
    "        ax1.plot(gen_calc_err, label=folder)\n",
    "        # show labels in a legend:\n",
    "        #ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "    #extent = (0, 512, 0, 512)\n",
    "    if include_raptor:  # subregion of the original image\n",
    "        axins = zoomed_inset_axes(ax1, 4, loc=\"upper center\")\n",
    "        #ax1.inset_axes(\n",
    "        #[0, 100, 0, 50],\n",
    "        #xlim=(x1, x2), ylim=(y1, y2), xticklabels=[], yticklabels=[])\n",
    "        gen_calc_err_min = 8000\n",
    "        gen_calc_err_min_max = -8000\n",
    "        for name in sorted(res):\n",
    "            gen_calc_err = pd.DataFrame(res[name][\"gen_calculated_error\"])\n",
    "            gen_calc_err = pd.concat([pd.DataFrame([raptor_res])[\"calculated_error_value\"], gen_calc_err])\n",
    "            gen_calc_err = gen_calc_err.reset_index(drop=True)\n",
    "            curr_min = min(gen_calc_err.values)\n",
    "            gen_calc_err_min = curr_min if curr_min < gen_calc_err_min else gen_calc_err_min\n",
    "            gen_calc_err_min_max = curr_min if curr_min > gen_calc_err_min_max else gen_calc_err_min_max\n",
    "            axins.plot(gen_calc_err, label=name)\n",
    "\n",
    "        axins.set_xlim(0, 18)\n",
    "        lower_bound = (gen_calc_err_min // 100) * 100\n",
    "        upper_bound = (gen_calc_err_min_max // 100) * 100 + 900\n",
    "        axins.set_ylim(lower_bound, upper_bound)\n",
    "        mark_inset(ax1, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
    "        #ax1.indicate_inset_zoom(axins, edgecolor=\"black\")\n",
    "    #fig.tight_layout()\n",
    "    ax1.legend()\n",
    "    plt.draw()\n",
    "\n",
    "    plt.savefig(f\"{'including_raptor_' if include_raptor else ''}{working_dir}_{dir_prefix}{img_format}.svg\",\n",
    "                format=\"svg\", dpi=1200)\n",
    "    plt.savefig(f\"{'including_raptor_' if include_raptor else ''}{working_dir}_{dir_prefix}{img_format}.pdf\",\n",
    "                bbox_inches=\"tight\")\n",
    "    print(f\"Saved as {'including_raptor_' if include_raptor else ''}{working_dir}_{dir_prefix}{img_format}.svg/pdf\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## compare different hyperparameters for Differential Evolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T08:18:19.526574600Z",
     "start_time": "2024-02-02T08:18:11.176881100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "working_dir = \"cmp_results\"\n",
    "dir_prefix = \"diff_cmp_f\"\n",
    "\n",
    "folders = [f for f in os.listdir(f\"{base_dir}/{working_dir}\") if f.startswith(dir_prefix)]\n",
    "\n",
    "# calculate raptor res for using the same optimization rules as in the json:\n",
    "raptor_res = None\n",
    "\n",
    "for include_raptor in [True, False]:\n",
    "    res = {}\n",
    "\n",
    "    if img_format != \"\":\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    else:\n",
    "        fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.set_title(\"Impact of the parameter f on the differential evolution optimization\")\n",
    "    ax1.set_xlabel(\"Generation\")\n",
    "    ax1.set_ylabel(\"Synthetic error\")\n",
    "\n",
    "    for folder in sorted(folders):\n",
    "        print(folder)\n",
    "        # load csv \"_ev_optimization_log.csv\":\n",
    "        content = load_json(f\"{base_dir}/{working_dir}/{folder}/diff_opt_state.json\")\n",
    "        if raptor_res is None:\n",
    "            raptor_res = recalculate_for_dist(content, use_raptor_dist=True).to_json()\n",
    "        gen_calc_err = pd.DataFrame(content[\"gen_calculated_error\"])\n",
    "        if include_raptor:\n",
    "            #df_diff = pd.concat([pd.DataFrame([raptor_res])[\"calculated_error_value\"], df_diff])\n",
    "            gen_calc_err = pd.concat([pd.DataFrame([raptor_res])[\"calculated_error_value\"], gen_calc_err])\n",
    "        res[folder.replace(dir_prefix + \"_\", \"\")] = content\n",
    "        ax1.plot(gen_calc_err, label=folder.replace(dir_prefix + \"_\", \"\"))\n",
    "        # show labels in a legend:\n",
    "        #ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "    #extent = (0, 512, 0, 512)\n",
    "    if include_raptor:  # subregion of the original image\n",
    "        axins = zoomed_inset_axes(ax1, 4, loc=\"upper center\")\n",
    "        #ax1.inset_axes(\n",
    "        #[0, 100, 0, 50],\n",
    "        #xlim=(x1, x2), ylim=(y1, y2), xticklabels=[], yticklabels=[])\n",
    "        gen_calc_err_min = 8000\n",
    "        gen_calc_err_min_max = -8000\n",
    "        for name in sorted(res):\n",
    "            gen_calc_err = pd.DataFrame(res[name][\"gen_calculated_error\"])\n",
    "            gen_calc_err = pd.concat([pd.DataFrame([raptor_res])[\"calculated_error_value\"], gen_calc_err])\n",
    "            gen_calc_err = gen_calc_err.reset_index(drop=True)\n",
    "            curr_min = min(gen_calc_err.values)\n",
    "            gen_calc_err_min = curr_min if curr_min < gen_calc_err_min else gen_calc_err_min\n",
    "            gen_calc_err_min_max = curr_min if curr_min > gen_calc_err_min_max else gen_calc_err_min_max\n",
    "            axins.plot(gen_calc_err, label=name.replace(dir_prefix + \"_\", \"\"))\n",
    "\n",
    "        axins.set_xlim(0, 20)\n",
    "        lower_bound = (gen_calc_err_min // 100) * 100\n",
    "        upper_bound = (gen_calc_err_min_max // 100) * 100 + 300\n",
    "        axins.set_ylim(lower_bound, upper_bound)\n",
    "\n",
    "        mark_inset(ax1, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
    "        #ax1.indicate_inset_zoom(axins, edgecolor=\"black\")\n",
    "    #fig.tight_layout()\n",
    "    ax1.legend()\n",
    "    plt.draw()\n",
    "\n",
    "    plt.savefig(f\"{'including_raptor_' if include_raptor else ''}{working_dir}_{dir_prefix}{img_format}.svg\",\n",
    "                format=\"svg\", dpi=1200)\n",
    "    plt.savefig(f\"{'including_raptor_' if include_raptor else ''}{working_dir}_{dir_prefix}{img_format}.pdf\",\n",
    "                bbox_inches=\"tight\")\n",
    "    print(f\"Saved as {'including_raptor_' if include_raptor else ''}{working_dir}_{dir_prefix}{img_format}.svg/pdf\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T08:18:27.904738100Z",
     "start_time": "2024-02-02T08:18:19.212936100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "dir_prefix = \"cmp_diff_opt_cr\"\n",
    "folders = [f for f in os.listdir(f\"{base_dir}/{working_dir}\") if f.startswith(dir_prefix)]\n",
    "\n",
    "# calculate raptor res for using the same optimization rules as in the json:\n",
    "raptor_res = None\n",
    "\n",
    "for include_raptor in [True, False]:\n",
    "    res = {}\n",
    "\n",
    "    if img_format != \"\":\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    else:\n",
    "        fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.set_title(\"Impact of the crossover rate on the differential evolution optimization\")\n",
    "    ax1.set_xlabel(\"Generation\")\n",
    "    ax1.set_ylabel(\"Synthetic error\")\n",
    "\n",
    "    for folder in sorted(folders):\n",
    "        print(folder)\n",
    "        # load csv \"_ev_optimization_log.csv\":\n",
    "        content = load_json(f\"{base_dir}/{working_dir}/{folder}/diff_opt_state.json\")\n",
    "        if raptor_res is None:\n",
    "            raptor_res = recalculate_for_dist(content, use_raptor_dist=True).to_json()\n",
    "        gen_calc_err = pd.DataFrame(content[\"gen_calculated_error\"])\n",
    "        if include_raptor:\n",
    "            #df_diff = pd.concat([pd.DataFrame([raptor_res])[\"calculated_error_value\"], df_diff])\n",
    "            gen_calc_err = pd.concat([pd.DataFrame([raptor_res])[\"calculated_error_value\"], gen_calc_err])\n",
    "        res[folder.replace(dir_prefix + \"_\", \"\")] = content\n",
    "        ax1.plot(gen_calc_err, label=folder.replace(dir_prefix + \"_\", \"\"))\n",
    "        # show labels in a legend:\n",
    "        #ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "    #extent = (0, 512, 0, 512)\n",
    "    if include_raptor:  # subregion of the original image\n",
    "        axins = zoomed_inset_axes(ax1, 4, loc=\"upper center\")\n",
    "        #ax1.inset_axes(\n",
    "        #[0, 100, 0, 50],\n",
    "        #xlim=(x1, x2), ylim=(y1, y2), xticklabels=[], yticklabels=[])\n",
    "        gen_calc_err_min = 8000\n",
    "        gen_calc_err_min_max = -8000\n",
    "        for name in sorted(res):\n",
    "            gen_calc_err = pd.DataFrame(res[name][\"gen_calculated_error\"])\n",
    "            gen_calc_err = pd.concat([pd.DataFrame([raptor_res])[\"calculated_error_value\"], gen_calc_err])\n",
    "            gen_calc_err = gen_calc_err.reset_index(drop=True)\n",
    "            curr_min = min(gen_calc_err.values)\n",
    "            gen_calc_err_min = curr_min if curr_min < gen_calc_err_min else gen_calc_err_min\n",
    "            gen_calc_err_min_max = curr_min if curr_min > gen_calc_err_min_max else gen_calc_err_min_max\n",
    "            axins.plot(gen_calc_err, label=name.replace(dir_prefix + \"_\", \"\"))\n",
    "\n",
    "        axins.set_xlim(0, 18)\n",
    "        lower_bound = (gen_calc_err_min // 100) * 100\n",
    "        upper_bound = (gen_calc_err_min_max // 100) * 100 + 200\n",
    "        axins.set_ylim(lower_bound, upper_bound)\n",
    "\n",
    "        mark_inset(ax1, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
    "        #ax1.indicate_inset_zoom(axins, edgecolor=\"black\")\n",
    "    #fig.tight_layout()\n",
    "    ax1.legend()\n",
    "    plt.draw()\n",
    "\n",
    "    plt.savefig(f\"{'including_raptor_' if include_raptor else ''}{working_dir}_{dir_prefix}{img_format}.svg\",\n",
    "                format=\"svg\", dpi=1200)\n",
    "    plt.savefig(f\"{'including_raptor_' if include_raptor else ''}{working_dir}_{dir_prefix}{img_format}.pdf\",\n",
    "                bbox_inches=\"tight\")\n",
    "    print(f\"Saved as {'including_raptor_' if include_raptor else ''}{working_dir}_{dir_prefix}{img_format}.svg/pdf\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T08:18:40.885773700Z",
     "start_time": "2024-02-02T08:18:27.909829500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "dir_prefix = \"GrdOpt_100_10_alpha\"\n",
    "\n",
    "folders = [f for f in os.listdir(f\"{base_dir}/{working_dir}\") if f.startswith(dir_prefix)]\n",
    "\n",
    "# calculate raptor res for using the same optimization rules as in the json:\n",
    "raptor_res = None\n",
    "\n",
    "for include_raptor in [True, False]:\n",
    "    res = {}\n",
    "\n",
    "    if img_format != \"\":\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    else:\n",
    "        fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.set_title(\"Impact of the parameter alpha for the gradient descent optimization\")\n",
    "    ax1.set_xlabel(\"Generation\")\n",
    "    ax1.set_ylabel(\"Synthetic error\")\n",
    "\n",
    "    for folder in sorted(folders):\n",
    "        print(folder)\n",
    "        # load csv \"_ev_optimization_log.csv\":\n",
    "        content = load_json(f\"{base_dir}/{working_dir}/{folder}/grd_opt_state.json\")\n",
    "        if raptor_res is None:\n",
    "            raptor_res = recalculate_for_dist(content, use_raptor_dist=True).to_json()\n",
    "        gen_calc_err = pd.DataFrame([x[\"calculated_error_value\"] for x in content[\"finished_gen\"]])\n",
    "        if include_raptor:\n",
    "            #df_diff = pd.concat([pd.DataFrame([raptor_res])[\"calculated_error_value\"], df_diff])\n",
    "            gen_calc_err = pd.concat([pd.DataFrame([raptor_res])[\"calculated_error_value\"], gen_calc_err])\n",
    "        res[folder.replace(dir_prefix + \"_\", \"\")] = content\n",
    "        ax1.plot(gen_calc_err, label=folder.replace(dir_prefix + \"_\", \"\"))\n",
    "        # show labels in a legend:\n",
    "        #ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "    #extent = (0, 512, 0, 512)\n",
    "    \"\"\"\n",
    "    if include_raptor: # subregion of the original image\n",
    "        axins = zoomed_inset_axes(ax1, 4, loc=\"upper center\")\n",
    "            #ax1.inset_axes(\n",
    "            #[0, 100, 0, 50],\n",
    "            #xlim=(x1, x2), ylim=(y1, y2), xticklabels=[], yticklabels=[])\n",
    "        for name in sorted(res):\n",
    "            gen_calc_err = pd.DataFrame(res[name][\"gen_calculated_error\"])\n",
    "            gen_calc_err = pd.concat([pd.DataFrame([raptor_res])[\"calculated_error_value\"], gen_calc_err])\n",
    "            gen_calc_err = gen_calc_err.reset_index(drop=True)\n",
    "            axins.plot(gen_calc_err, label=name.replace(dir_prefix+\"_\",\"\"))\n",
    "\n",
    "        axins.set_xlim(0,20)\n",
    "        axins.set_ylim(2500,3200)\n",
    "\n",
    "        mark_inset(ax1, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
    "        #ax1.indicate_inset_zoom(axins, edgecolor=\"black\")\n",
    "    #fig.tight_layout()\n",
    "    \"\"\"\n",
    "    # set legend position bottom right:\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    plt.draw()\n",
    "\n",
    "    plt.savefig(f\"{'including_raptor_' if include_raptor else ''}{working_dir}_{dir_prefix}{img_format}.svg\",\n",
    "                format=\"svg\", dpi=1200)\n",
    "    plt.savefig(f\"{'including_raptor_' if include_raptor else ''}{working_dir}_{dir_prefix}{img_format}.pdf\",\n",
    "                bbox_inches=\"tight\")\n",
    "    print(f\"Saved as {'including_raptor_' if include_raptor else ''}{working_dir}_{dir_prefix}{img_format}.svg/pdf\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T08:18:41.302994900Z",
     "start_time": "2024-02-02T08:18:40.884755800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# working_dir = \"/from_server2/results_Dorn_only\"\n",
    "working_dir = \"results_final_same_params\"\n",
    "filename1 = \"results_evo/evo_opt_state.json\"\n",
    "filename2 = \"results_diff/diff_opt_state.json/diff_opt_state.json\"\n",
    "filename3 = \"GrdOpt_250_10_0.001results_grd/grd_opt_state.json\"\n",
    "\n",
    "json_evo = load_json(f\"{base_dir}/{working_dir}/{filename1}\")\n",
    "json_diff = load_json(f\"{base_dir}/{working_dir}/{filename2}\")\n",
    "json_grd = load_json(f\"{base_dir}/{working_dir}/{filename3}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T08:18:41.321434800Z",
     "start_time": "2024-02-02T08:18:41.304012700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "gen_avg_err = []\n",
    "gen_avg_over = []\n",
    "gen_calc_err = []\n",
    "gen_clean_deg_len = []\n",
    "for gen in json_grd[\"finished_gen\"]:\n",
    "    gen_avg_err.append(gen[\"avg_err\"])\n",
    "    gen_clean_deg_len.append(gen[\"clean_deg_len\"])\n",
    "    gen_avg_over.append(gen[\"overhead\"])\n",
    "    gen_calc_err.append(gen[\"calculated_error_value\"])\n",
    "json_grd[\"gen_avg_err\"] = gen_avg_err\n",
    "json_grd[\"gen_avg_over\"] = gen_avg_over\n",
    "json_grd[\"gen_calculated_error\"] = gen_calc_err"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T08:18:41.325524200Z",
     "start_time": "2024-02-02T08:18:41.310166600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# assert that the results are comparable:\n",
    "dirty = False\n",
    "for key in json_diff.keys():\n",
    "    if \"_fac\" in key:\n",
    "        if json_evo[key] != json_diff[key] or json_grd[key] != json_diff[key]:\n",
    "            dirty = True\n",
    "            print(f\"{key}: {json_evo[key]} (evo) != {json_diff[key]} (diff)\")\n",
    "if not dirty:\n",
    "    print(\"All hyperparameters are equal\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T08:18:43.770798600Z",
     "start_time": "2024-02-02T08:18:41.328582800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# avg_unrecovered packets and synthetic error value for Evolutionary Optimization\n",
    "for mode in [\"syn_err\", \"overhead\", \"avg_err\"]:\n",
    "    if img_format != \"\":\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    else:\n",
    "        fig, ax1 = plt.subplots()\n",
    "    if mode == \"syn_err\":\n",
    "        dataframe_name = \"gen_calculated_error\"\n",
    "        dist_name = \"calculated_error_value\"\n",
    "        ylabel = \"Average synthetic error\"\n",
    "        title = \"Average synthetic error for the used optimization methods\"\n",
    "    elif mode == \"overhead\":\n",
    "        dataframe_name = \"gen_avg_over\"\n",
    "        dist_name = \"overhead\"\n",
    "        ylabel = \"Average overhead\"\n",
    "        title = \"Average overhead for the used optimization methods\"\n",
    "    elif mode == \"avg_err\":\n",
    "        dataframe_name = \"gen_avg_err\"\n",
    "        dist_name = \"avg_err\"\n",
    "        ylabel = \"average error\"\n",
    "        title = \"Average error probability for the used optimization methods\"\n",
    "    #elif mode == \"clean_deg_len\":\n",
    "    #    dataframe_name = \"gen_clean_deg_len\"\n",
    "    #    dist_name = \"clean_deg_len\"\n",
    "    #    ylabel = \"average rule abiding packets\"\n",
    "    #    title = \"Average rule abiding packets for the used optimization methods\"\n",
    "    else:\n",
    "        raise ValueError(\"Unknown mode\")\n",
    "\n",
    "    ax1.set_title(title)\n",
    "    ax1.set_xlabel(\"Generation\")\n",
    "    ax1.set_ylabel(ylabel)\n",
    "    df_evo = pd.DataFrame(json_evo[dataframe_name][:250])\n",
    "    df_evo = pd.concat([pd.DataFrame([raptor_res])[dist_name], df_evo])\n",
    "    df_evo = df_evo.reset_index(drop=True)\n",
    "    #df_evo_calc_err = df_evo\n",
    "    ax1.plot(df_evo, color=\"tab:red\", label=\"Evolutionary optimization\")\n",
    "\n",
    "    df_diff = pd.DataFrame(json_diff[dataframe_name][:250])\n",
    "    df_diff = pd.concat([pd.DataFrame([raptor_res])[dist_name], df_diff])\n",
    "    df_diff = df_diff.reset_index(drop=True)\n",
    "    df_diff_calc_err = df_diff\n",
    "    ax1.plot(df_diff, color=\"tab:blue\", label=\"Differential evolution\")\n",
    "    #ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "    df_grd = pd.DataFrame(json_grd[dataframe_name][:250])\n",
    "    df_grd = pd.concat([pd.DataFrame([raptor_res])[dist_name], df_grd])\n",
    "    df_grd = df_grd.reset_index(drop=True)\n",
    "\n",
    "    #df_grd = pd.concat([pd.DataFrame([raptor_res])[\"calculated_error_value\"], df_grd])\n",
    "    #df_grd_calc_err = df_grd\n",
    "    #ax1.plot(df_grd, color=\"tab:orange\", label=\"Differential Evolution\")\n",
    "    #ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "    ax1.legend([\"Evolutionary optimization\", \"Differential evolution\"])  #, \"Gradient Descent\"])\n",
    "\n",
    "    pdf_filename = f\"{working_dir}_{filename1.split('/')[0]}_{filename2.split('/')[0]}_comparison_{mode}{img_format}\"\n",
    "    plt.savefig(pdf_filename + \".pdf\", bbox_inches=\"tight\")\n",
    "    plt.savefig(pdf_filename + \".svg\", format=\"svg\", dpi=1200)\n",
    "    print(f\"Saved file as: {pdf_filename}.pdf/.svg\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T08:18:43.780000500Z",
     "start_time": "2024-02-02T08:18:43.770798600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
